{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = TextLoader('nvda_news_1.txt')\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'nvda_news_1.txt'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders.csv_loader import CSVLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = CSVLoader('movies.csv')\n",
    "data = loader.load()\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.documents.base.Document"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'movie_id: 101\\ntitle: K.G.F: Chapter 2\\nindustry: Bollywood\\nrelease_year: 2022\\nimdb_rating: 8.4\\nstudio: Hombale Films\\nlanguage_id: 3\\nbudget: 1\\nrevenue: 12.5\\nunit: Billions\\ncurrency: INR'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'movies.csv', 'row': 0}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import UnstructuredURLLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader1 = UnstructuredURLLoader(urls = [\n",
    "    \"https://medium.com/@davidfagb/guide-to-transfer-learning-in-deep-learning-1f685db1fc94\",\n",
    "    \"https://geometry.cs.ucl.ac.uk/projects/2016/texture_transfer/\",\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = loader1.load()\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'https://medium.com/@davidfagb/guide-to-transfer-learning-in-deep-learning-1f685db1fc94'}, page_content='Open in app\\n\\nSign in\\n\\nWrite\\n\\nSign in\\n\\nGuide To Transfer Learning in Deep Learning\\n\\nDavid Fagbuyiro\\n\\nFollow\\n\\n14 min read\\n\\nApr 20, 2024\\n\\n--\\n\\nIn this guide, we will cover what transfer learning is, and the main approaches to transfer learning in deep learning.\\n\\nWhat is Transfer Learning\\n\\nTransfer learning is an approach to machine learning where a model trained on one task is used as the starting point for a model on a new task. This is done by transferring the knowledge that the first model has learned about the features of the data to the second model.\\n\\nIn deep learning, transfer learning is often used to solve problems with limited data. This is because deep learning models typically require a large amount of data to train, which can be difficult or expensive to obtain.\\n\\nWhy Use Transfer Learning?\\n\\nHere are some reasons why you might want to use transfer learning:\\n\\nTo save time and resources: Training a deep learning model from scratch can be time-consuming and computationally expensive. Transfer learning can help you save time and resources by starting with a model that has already been trained on a large dataset.\\n\\nTo improve model performance: Transfer learning can help you improve the performance of your model by transferring the knowledge that the pre-trained model has learned about the features of the data. This can be especially helpful if you have limited data for your target task.\\n\\nTo solve problems with limited data: Transfer learning can be used to solve problems with limited data by transferring the knowledge that the pre-trained model has learned about the features of the data. This can be done by using feature extraction or fine-tuning.\\n\\nTypes of Transfer Learning\\n\\nTransfer learning can be classified into two types:\\n\\nFeature extraction: In feature extraction, the pre-trained model is used to extract features from the data. These features are then used to train a new model on the target task. This is a good approach if you have limited data for the target task.\\n\\nExample:\\n\\nimport numpy as np\\nfrom sklearn.feature_extraction.text import TfidfVectorizer\\nfrom sklearn.linear_model import LogisticRegression\\n\\n# Load the pre-trained model\\npretrained_model = TfidfVectorizer()\\npretrained_model.fit([\\'This is a positive review\\', \\'This is a negative review\\'])\\n\\n# Extract features from the data\\nX_train = pretrained_model.transform([\\'I love this product!\\', \\'This product is terrible.\\'])\\n\\n# Train a new model on the target task\\ny_train = np.array([1, 0])\\nclf = LogisticRegression()\\nclf.fit(X_train, y_train)\\n\\n# Make predictions on new data\\nX_new = pretrained_model.transform([\\'This product is the best!\\'])\\ny_pred = clf.predict(X_new)\\n\\n# Print the prediction\\nprint(y_pred)  \\n\\nThe Output:\\n\\n[1]\\n\\nThis approach is useful if you have limited data for the target task. For example, if you have a small number of labeled reviews, you can use a pre-trained model to extract features from the reviews.\\n\\nFine-tuning: Fine-tuning is a machine learning technique in which a pre-trained model is further trained on a new dataset to improve its performance on a specific task. The pre-trained model is typically trained on a large dataset of general data, while the new dataset is specific to the task at hand.\\n\\nBelow is a code example of fine-tuning a pre-trained language model (LLM) to classify text as either positive or negative:\\n\\nimport torch\\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\\n\\n# Load the pre-trained LLM\\ntokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\\nmodel = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")\\n\\n# Define the hyperparameters\\nlearning_rate = 5e-5\\nepochs = 5\\n\\n# Load the training data\\ntrain_data = torch.load(\"train_data.pt\")\\n\\n# Create the optimizer\\noptimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\\n\\n# Train the model\\nfor epoch in range(epochs):\\n    for batch in train_data:\\n        # Forward pass\\n        outputs = model(batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"], labels=batch[\"labels\"])\\n\\n        # Calculate the loss\\n        loss = outputs.loss\\n\\n        # Backward pass\\n        optimizer.zero_grad()\\n        loss.backward()\\n        optimizer.step()\\n\\n# Save the fine-tuned model\\ntorch.save(model.state_dict(), \"fine-tuned_model.pt\")\\n\\nThis code will fine-tune the pre-trained DistilBERT LLM on the train_data dataset. The train_data dataset should be a PyTorch DataLoader object containing batches of text sequences and their corresponding labels (either positive or negative).\\n\\nTo use the fine-tuned model, you can simply load it and then call the model() method with the text sequence that you want to classify. For example:\\n\\n# Load the fine-tuned model\\nmodel = torch.load(\"fine-tuned_model.pt\")\\n\\n# Classify a text sequence\\ntext_sequence = \"This is a positive review.\"\\noutputs = model(tokenizer(text_sequence, return_tensors=\"pt\")[\"input_ids\"])\\n\\n# Get the predicted label\\npredicted_label = outputs.logits.argmax(-1)\\n\\n# Print the predicted label\\nprint(predicted_label)\\n\\nThe code above will print the predicted label for the text sequence This is a positive review.. In this case, the predicted label is 1, which corresponds to the positive class.\\n\\nFine-tuning can be used to improve the performance of pre-trained LLMs on a wide variety of tasks, such as sentiment analysis, question answering, and text summarization.\\n\\nMulti-task learning: In multi-task learning, the pre-trained model is trained on multiple tasks simultaneously. This can help to improve the performance of the model on each task.\\n\\nimport torch\\nimport torch.nn as nn\\n\\n# Define the pre-trained model\\nclass PreTrainedModel(nn.Module):\\n    def __init__(self):\\n        super().__init__()\\n        self.features = nn.Sequential(\\n            nn.Linear(100, 128),\\n            nn.ReLU(),\\n            nn.Linear(128, 64),\\n            nn.ReLU()\\n        )\\n\\n    def forward(self, x):\\n        x = self.features(x)\\n        return x\\n\\n# Define the multi-task model\\nclass MultiTaskModel(nn.Module):\\n    def __init__(self, pre_trained_model):\\n        super().__init__()\\n        self.pre_trained_model = pre_trained_model\\n\\n        # Define the task-specific heads\\n        self.head1 = nn.Linear(64, 1)\\n        self.head2 = nn.Linear(64, 2)\\n\\n    def forward(self, x):\\n        x = self.pre_trained_model(x)\\n\\n        # Compute the predictions for each task\\n        y1 = self.head1(x)\\n        y2 = self.head2(x)\\n\\n        return y1, y2\\n\\n# Load the pre-trained model\\npre_trained_model = PreTrainedModel()\\npre_trained_model.load_state_dict(torch.load(\\'pre_trained_model.pt\\'))\\n\\n# Define the multi-task model\\nmulti_task_model = MultiTaskModel(pre_trained_model)\\n\\n# Train the multi-task model on multiple tasks simultaneously\\noptimizer = torch.optim.Adam(multi_task_model.parameters())\\n\\n# Define the loss function\\nloss_fn = nn.MSELoss()\\n\\n# Train the model for 100 epochs\\nfor epoch in range(100):\\n    # Load a batch of data for each task\\n    x1, y1 = ...\\n    x2, y2 = ...\\n\\n    # Compute the predictions for each task\\n    y1_pred, y2_pred = multi_task_model(x1, x2)\\n\\n    # Compute the loss\\n    loss = loss_fn(y1_pred, y1) + loss_fn(y2_pred, y2)\\n\\n    # Backpropagate the loss\\n    optimizer.zero_grad()\\n    loss.backward()\\n    optimizer.step()\\n\\n# Evaluate the multi-task model on each task\\ny1_pred, y2_pred = multi_task_model(x1, x2)\\n\\n# Compute the accuracy\\nacc1 = (y1_pred.argmax(1) == y1).float().mean()\\nacc2 = (y2_pred.argmax(1) == y2).float().mean()\\n\\nprint(\\'Accuracy on task 1:\\', acc1)\\nprint(\\'Accuracy on task 2:\\', acc2)\\n\\nDomain adaptation: In domain adaptation, the pre-trained model is adapted to a new domain. This can be done by adjusting the weights of the model’s layers or by using a technique called data augmentation.\\n\\nimport numpy as np\\nimport tensorflow as tf\\n\\n# Load the source and target datasets\\n(x_source, y_source), (x_target, y_target) = tf.keras.datasets.mnist.load_data()\\n\\n# Preprocess the data\\nx_source = x_source / 255.\\nx_target = x_target / 255.\\n\\n# Define the pre-trained model\\nmodel = tf.keras.models.Sequential([\\n  tf.keras.layers.Flatten(input_shape=(28, 28)),\\n  tf.keras.layers.Dense(128, activation=\\'relu\\'),\\n  tf.keras.layers.Dense(10, activation=\\'softmax\\')\\n])\\n\\n# Compile the model\\nmodel.compile(optimizer=\\'adam\\',\\n              loss=\\'sparse_categorical_crossentropy\\',\\n              metrics=[\\'accuracy\\'])\\n\\n# Train the model on the source dataset\\nmodel.fit(x_source, y_source, epochs=10)\\n\\n# Adapt the model to the target domain using data augmentation\\ndata_augmentation = tf.keras.Sequential([\\n  tf.keras.layers.RandomFlip(\\'horizontal\\'),\\n  tf.keras.layers.RandomRotation(0.2),\\n])\\n\\n# Adapt the model to the target dataset\\nmodel.fit(data_augmentation(x_target), y_target, epochs=10)\\n\\n# Evaluate the model on the target dataset\\ntest_loss, test_accuracy = model.evaluate(x_target, y_target)\\n\\nprint(\\'Test loss:\\', test_loss)\\nprint(\\'Test accuracy:\\', test_accuracy)\\n\\n# Zero-shot learning: In zero-shot learning, the model is trained on a set of classes that it has never seen before. \\n# This is accomplished through the use of a method known as word embedding.\\n\\nWhat Is a Pre-Trained Model?\\n\\nA pre-trained model is a machine learning model that has been trained on a large dataset of data. This dataset is typically much larger than the dataset that will be used to train the final model. The pre-trained model learns to extract features from the data, and these features can be used to train the final model more quickly and efficiently.\\n\\nPopular Pre-Trained Architectures\\n\\nThere are many popular pre-trained architectures, but some of the most common include:\\n\\nVGG (Visual Geometry Group): VGG is a family of convolutional neural networks that were first introduced in 2014. They are known for their simplicity and efficiency, and they have been used for a variety of tasks, including image classification, object detection, and segmentation.\\n\\nResNet (Residual Network): ResNet is a family of convolutional neural networks that were introduced in 2015. They are known for their ability to learn deeper features, and they have been shown to achieve state-of-the-art results on a variety of tasks.\\n\\nBERT (Bidirectional Encoder Representations from Transformers): BERT is a language model that was introduced in 2018. It is known for its ability to learn long-range dependencies in text, and it has been used for a variety of natural language processing tasks, including question-answering, sentiment analysis, and text summarization.\\n\\nThese are just a few of the many popular pre-trained architectures. The best architecture for a particular task will depend on the specific requirements of the task.\\n\\nWhat is Transferable Knowledge?\\n\\nTransferable knowledge in the context of transfer learning refers to the information, patterns, and representations learned by a machine learning model during training on one task or domain that can be effectively applied to improve performance on a different, often related, task or domain.\\n\\nLow-level Features vs. High-level Semantics\\n\\nIn transfer learning, models can transfer both low-level features and high-level semantics from a source task/domain to a target task/domain.\\n\\n1. Low-level Features: These are basic, primitive, and often raw data representations. In image processing, low-level features could include edges, colors, textures, or local patterns. In natural language processing, they might involve word embeddings or character-level features. Low-level features capture fundamental characteristics of the data that are often reusable in various contexts.\\n\\n2. High-level Semantics: High-level semantics represent more abstract and complex concepts or meanings within the data. In image analysis, high-level semantics could involve recognizing objects, scenes, or context. In natural language understanding, it might entail understanding sentiment, intent, or named entities.\\n\\nDomain-specific vs. Generic Knowledge\\n\\nTransfer learning can involve transferring both domain-specific and generic knowledge.\\n\\n1. Domain-specific Knowledge: This refers to knowledge and information that is specific to a particular task or domain. For example, medical image analysis might involve domain-specific knowledge related to anatomical structures or disease patterns. Transfer learning can adapt such knowledge to improve performance on similar medical imaging tasks.\\n\\n2. Generic Knowledge: Generic knowledge, on the other hand, is more general and broadly applicable. It includes foundational concepts and representations that are not tied to a specific task or domain. For instance, knowledge about spatial relationships, language syntax, or image textures can be considered generic. This type of knowledge can be transferred across a wide range of tasks and domains to aid in learning.\\n\\nTask Similarity & Domains\\n\\nHigh Task Similarity: When two tasks are highly similar, it means their objectives and desired outputs are closely related. For example, recognizing different breeds of dogs and recognizing different species of cats have high task similarity because they both involve image classification tasks related to animals.\\n\\nLow Task Similarity: In contrast, low task similarity indicates that the two tasks have distinct objectives. For instance, sentiment analysis of product reviews and image recognition of landmarks have low task similarity as they involve different data types and objectives.\\n\\nHigh Domain Similarity: High domain similarity suggests that two datasets or data distributions are similar in terms of their characteristics, such as data format, modality, or context. For example, medical X-ray images from different hospitals may have high domain similarity if they share similar image quality and medical conditions.\\n\\nLow Domain Similarity: Low domain similarity means that the datasets are dissimilar. For instance, medical X-ray images and satellite images of landscapes have low domain similarity because they come from entirely different domains with distinct data characteristics.\\n\\nFine-tuning vs. Feature Extraction\\n\\nIn transfer learning, fine-tuning and feature extraction are two common techniques to adapt a pre-trained model to a new task or domain:\\n\\nFine-tuning: Fine-tuning involves taking a pre-trained model (often on a source task) and training it further on a target task. During fine-tuning, the model’s weights are updated using the target task’s data while retaining some knowledge from the source task. Fine-tuning is particularly useful when the source and target tasks are closely related.\\n\\nFeature Extraction: Feature extraction refers to using a pre-trained model as a fixed feature extractor. Instead of modifying the model’s weights, the model is used to extract relevant features from the input data. These features can then be fed into a new classifier or model specific to the target task.\\n\\nSame-domain vs. Cross-domain Transfer\\n\\nSame-domain and cross-domain transfer refer to the relationship between the source and target domains in transfer learning:\\n\\nSame-domain Transfer: In same-domain transfer, the source and target domains are closely related or identical. This means that the data distribution, data characteristics, and context are similar between the source and target domains. Same-domain transfer is typically easier and tends to yield better results because the knowledge learned in one domain can be directly applied to the other.\\n\\nCross-domain Transfer: Cross-domain transfer involves transferring knowledge from a source domain to a target domain that is different. This type of transfer learning is more challenging because differences in data distribution, data modality, or context must be addressed. Techniques like domain adaptation and fine-tuning are often used to bridge the gap between the source and target domains in cross-domain transfer.\\n\\nSteps to Implement Transfer Learning\\n\\nHere are the key steps to implement transfer learning in a nutshell:\\n\\n1. Dataset Preparation: Begin by collecting and preprocessing your dataset. Ensure it is well-organized and contains labeled data for your target task. Typically, transfer learning works best when you have a smaller dataset, and it’s essential to split it into training, validation, and testing sets.\\n\\n2. Model Selection & Architecture: Choose a pre-trained deep learning model that suits your problem. Common choices include VGG, ResNet, Inception, and BERT, depending on whether it’s an image classification, object detection, or natural language processing task. Next, adapt the architecture of the selected model to match the number of classes or labels in your dataset.\\n\\n3. Transfer Strategy: Decide on the transfer learning strategy. There are two primary approaches which are: feature extraction and fine-tuning.\\n\\n4. Hyperparameter Tuning: Optimize hyperparameters such as learning rate, batch size, and the number of epochs. You can use techniques like grid search or random search to find the best combination of hyperparameters. This step is crucial for achieving optimal model performance.\\n\\n5. Training & Evaluation: Train the modified model on your dataset using the chosen hyperparameters. Monitor the training process by tracking metrics on the validation set. To prevent overfitting, consider using techniques like early stopping or dropout. Finally, evaluate the model’s performance on the test set using appropriate evaluation metrics, depending on your task (accuracy, F1-score, mean squared error, etc.).\\n\\nChallenges and Considerations\\n\\nWhile it offers numerous advantages, it also comes with its own set of challenges and considerations that practitioners must be aware of.\\n\\n1. Dataset Bias & Mismatch: Transfer learning relies heavily on the assumption that the source and target domains share some similarities. However, in real-world scenarios, datasets can exhibit bias or mismatch in terms of distribution, quality, or context.\\n\\n2. Overfitting & Generalization: One of the key challenges in transfer learning is finding the right balance between overfitting and generalization. Transferring too much knowledge from the source domain may lead to overfitting on the target domain, while transferring too little may hinder generalization.\\n\\n3. Catastrophic Forgetting: Transfer learning often involves fine-tuning a pre-trained model on a new task. However, this process can lead to the phenomenon known as catastrophic forgetting, where the model forgets important information from the source domain while adapting to the target domain. Strategies like progressive learning or elastic weight consolidation can help alleviate this issue.\\n\\n4. Ethical & Privacy Concerns: Transfer learning can be problematic when dealing with sensitive data, such as medical records or personal information. Care must be taken to ensure that transferred knowledge doesn’t violate privacy or ethical standards.\\n\\n5. Computational Resources: Large pre-trained models, such as those in natural language processing, demand substantial computational resources for training and fine-tuning. Smaller organizations or researchers with limited access to high-performance computing may face challenges in implementing transfer learning effectively.\\n\\nPractical Applications of Transfer Learning\\n\\nHere, we explore some practical applications of transfer learning in different fields with code examples:\\n\\nImage Classification: Transfer learning can be used to train models to classify images of different objects, such as cats, dogs, cars, and furniture. This can be used in a variety of applications, such as product search, image tagging, and medical imaging.\\n\\nimport tensorflow as tf\\n\\n# Load the pre-trained model\\nmodel = tf.keras.applications.MobileNetV2(weights=\\'imagenet\\')\\n\\n# Freeze the lower layers\\nfor layer in model.layers[:10]:\\n    layer.trainable = False\\n\\n# Add a new trainable layer for classification\\nnew_layer = tf.keras.layers.Dense(10, activation=\\'softmax\\')\\nmodel.add(new_layer)\\n\\n# Compile the model\\nmodel.compile(optimizer=\\'adam\\', loss=\\'sparse_categorical_crossentropy\\', metrics=[\\'accuracy\\'])\\n\\n# Train the model on your dataset\\n# ...\\n\\n# Evaluate the model on your test dataset\\n# ...\\n\\n# Save the model\\nmodel.save(\\'my_model.h5\\')\\n\\nNatural Language Processing: Transfer learning can be used to train models to perform natural language processing tasks, such as text classification, sentiment analysis, and machine translation. This can be used in a variety of applications, such as spam filtering, customer service, and social media analysis.\\n\\nimport tensorflow as tf\\n\\n# Load the pre-trained model\\nmodel = tf.keras.applications.bert.BertModel.from_pretrained(\\'bert-base-uncased\\')\\n\\n# Add a new trainable layer for sentiment analysis\\nnew_layer = tf.keras.layers.Dense(2, activation=\\'softmax\\')\\nmodel.add(new_layer)\\n\\n# Compile the model\\nmodel.compile(optimizer=\\'adam\\', loss=\\'sparse_categorical_crossentropy\\', metrics=[\\'accuracy\\'])\\n\\n# Train the model on your dataset\\n# ...\\n\\n# Evaluate the model on your test dataset\\n# ...\\n\\n# Make a prediction for a sentence\\nsentence = \\'I love this movie!\\'\\n\\n# Tokenize the sentence\\ntokens = model.tokenize(sentence)\\n\\n# Encode the tokens to hidden states\\nhidden_states = model([tokens])\\n\\n# Extract the output of the last layer\\noutput = hidden_states[:, -1, :]\\n\\n# Make a prediction\\nprediction = new_layer(output)\\n\\n# Print the prediction\\nprint(prediction)\\n\\nRecommendation Systems: Transfer learning can be used to train models to recommend products, movies, and other items to users. This can be used to improve the user experience on e-commerce websites and streaming services.\\n\\nimport tensorflow as tf\\n\\n# Load the pre-trained model\\nmodel = tf.keras.models.load_model(\\'my_model.h5\\')\\n\\n# Make a prediction for a user\\nuser_id = 1\\nitem_ids = [2, 3, 4]\\n\\npredictions = model.predict([user_id, item_ids])\\n\\n# Recommend the top 5 items\\nrecommended_item_ids = predictions.argsort()[::-1][:5]\\n\\n# Print the recommended items\\nprint(recommended_item_ids)\\n\\nFraud Detection: Transfer learning can be used to train models to detect fraudulent transactions and other types of fraud. This can be used to protect businesses and consumers from financial losses.\\n\\nimport tensorflow as tf\\n\\n# Load the pre-trained model\\nmodel = tf.keras.models.load_model(\\'my_model.h5\\')\\n\\n# Make a prediction for a transaction\\ntransaction_data = {\\n    \\'amount\\': 1000,\\n    \\'merchant\\': \\'Amazon\\',\\n    \\'card_type\\': \\'Visa\\',\\n    \\'country\\': \\'US\\'\\n}\\n\\nprediction = model.predict(transaction_data)\\n\\n# If the prediction is greater than 0.5, the transaction is likely to be fraudulent\\nif prediction > 0.5:\\n    print(\\'The transaction is likely to be fraudulent.\\')\\nelse:\\n    print(\\'The transaction is likely to be legitimate.\\')\\n\\nMedical Diagnosis: Transfer learning can be used to train models to diagnose diseases and other medical conditions. This can be used to improve the accuracy and efficiency of medical diagnosis.\\n\\nimport tensorflow as tf\\n\\n# Load the pre-trained model\\nmodel = tf.keras.models.load_model(\\'my_model.h5\\')\\n\\n# Make a prediction for an image\\nimage_path = \\'chest_xray.jpg\\'\\n\\nimage = tf.keras.preprocessing.image.load_img(image_path, target_size=(224, 224))\\nimage = tf.keras.preprocessing.image.img_to_array(image)\\nimage = tf.expand_dims(image, axis=0)\\n\\nprediction = model.predict(image)\\n\\n# If the prediction is greater than 0.5, the image is likely to show pneumonia\\nif prediction > 0.5:\\n    print(\\'The image is likely to show pneumonia.\\')\\nelse:\\n    print(\\'The image is likely to be normal.\\')\\n\\nConclusion\\n\\nThrough this guide, we’ve explored all you need to know about transfer learning, including different transfer learning scenarios, popular pre-trained models, and the steps involved in adapting them for various tasks with code examples.\\n\\nArtificial Intelligence\\n\\nProgramming\\n\\nMachine Learning\\n\\nLlm\\n\\n--\\n\\n--\\n\\nWritten by David Fagbuyiro\\n\\n23 Followers\\n\\nTechnical writer\\n\\nHelp\\n\\nStatus\\n\\nAbout\\n\\nCareers\\n\\nPress\\n\\nBlog\\n\\nPrivacy\\n\\nTerms\\n\\nText to speech\\n\\nTeams')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Open in app\\n\\nSign in\\n\\nWrite\\n\\nSign in\\n\\nGuide To Transfer Learning in Deep Learning\\n\\nDavid Fagbuyiro\\n\\nFollow\\n\\n14 min read\\n\\nApr 20, 2024\\n\\n--\\n\\nIn this guide, we will cover what transfer learning is, and the main approaches to transfer learning in deep learning.\\n\\nWhat is Transfer Learning\\n\\nTransfer learning is an approach to machine learning where a model trained on one task is used as the starting point for a model on a new task. This is done by transferring the knowledge that the first model has learned about the features of the data to the second model.\\n\\nIn deep learning, transfer learning is often used to solve problems with limited data. This is because deep learning models typically require a large amount of data to train, which can be difficult or expensive to obtain.\\n\\nWhy Use Transfer Learning?\\n\\nHere are some reasons why you might want to use transfer learning:\\n\\nTo save time and resources: Training a deep learning model from scratch can be time-consuming and computationally expensive. Transfer learning can help you save time and resources by starting with a model that has already been trained on a large dataset.\\n\\nTo improve model performance: Transfer learning can help you improve the performance of your model by transferring the knowledge that the pre-trained model has learned about the features of the data. This can be especially helpful if you have limited data for your target task.\\n\\nTo solve problems with limited data: Transfer learning can be used to solve problems with limited data by transferring the knowledge that the pre-trained model has learned about the features of the data. This can be done by using feature extraction or fine-tuning.\\n\\nTypes of Transfer Learning\\n\\nTransfer learning can be classified into two types:\\n\\nFeature extraction: In feature extraction, the pre-trained model is used to extract features from the data. These features are then used to train a new model on the target task. This is a good approach if you have limited data for the target task.\\n\\nExample:\\n\\nimport numpy as np\\nfrom sklearn.feature_extraction.text import TfidfVectorizer\\nfrom sklearn.linear_model import LogisticRegression\\n\\n# Load the pre-trained model\\npretrained_model = TfidfVectorizer()\\npretrained_model.fit([\\'This is a positive review\\', \\'This is a negative review\\'])\\n\\n# Extract features from the data\\nX_train = pretrained_model.transform([\\'I love this product!\\', \\'This product is terrible.\\'])\\n\\n# Train a new model on the target task\\ny_train = np.array([1, 0])\\nclf = LogisticRegression()\\nclf.fit(X_train, y_train)\\n\\n# Make predictions on new data\\nX_new = pretrained_model.transform([\\'This product is the best!\\'])\\ny_pred = clf.predict(X_new)\\n\\n# Print the prediction\\nprint(y_pred)  \\n\\nThe Output:\\n\\n[1]\\n\\nThis approach is useful if you have limited data for the target task. For example, if you have a small number of labeled reviews, you can use a pre-trained model to extract features from the reviews.\\n\\nFine-tuning: Fine-tuning is a machine learning technique in which a pre-trained model is further trained on a new dataset to improve its performance on a specific task. The pre-trained model is typically trained on a large dataset of general data, while the new dataset is specific to the task at hand.\\n\\nBelow is a code example of fine-tuning a pre-trained language model (LLM) to classify text as either positive or negative:\\n\\nimport torch\\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\\n\\n# Load the pre-trained LLM\\ntokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\\nmodel = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")\\n\\n# Define the hyperparameters\\nlearning_rate = 5e-5\\nepochs = 5\\n\\n# Load the training data\\ntrain_data = torch.load(\"train_data.pt\")\\n\\n# Create the optimizer\\noptimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\\n\\n# Train the model\\nfor epoch in range(epochs):\\n    for batch in train_data:\\n        # Forward pass\\n        outputs = model(batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"], labels=batch[\"labels\"])\\n\\n        # Calculate the loss\\n        loss = outputs.loss\\n\\n        # Backward pass\\n        optimizer.zero_grad()\\n        loss.backward()\\n        optimizer.step()\\n\\n# Save the fine-tuned model\\ntorch.save(model.state_dict(), \"fine-tuned_model.pt\")\\n\\nThis code will fine-tune the pre-trained DistilBERT LLM on the train_data dataset. The train_data dataset should be a PyTorch DataLoader object containing batches of text sequences and their corresponding labels (either positive or negative).\\n\\nTo use the fine-tuned model, you can simply load it and then call the model() method with the text sequence that you want to classify. For example:\\n\\n# Load the fine-tuned model\\nmodel = torch.load(\"fine-tuned_model.pt\")\\n\\n# Classify a text sequence\\ntext_sequence = \"This is a positive review.\"\\noutputs = model(tokenizer(text_sequence, return_tensors=\"pt\")[\"input_ids\"])\\n\\n# Get the predicted label\\npredicted_label = outputs.logits.argmax(-1)\\n\\n# Print the predicted label\\nprint(predicted_label)\\n\\nThe code above will print the predicted label for the text sequence This is a positive review.. In this case, the predicted label is 1, which corresponds to the positive class.\\n\\nFine-tuning can be used to improve the performance of pre-trained LLMs on a wide variety of tasks, such as sentiment analysis, question answering, and text summarization.\\n\\nMulti-task learning: In multi-task learning, the pre-trained model is trained on multiple tasks simultaneously. This can help to improve the performance of the model on each task.\\n\\nimport torch\\nimport torch.nn as nn\\n\\n# Define the pre-trained model\\nclass PreTrainedModel(nn.Module):\\n    def __init__(self):\\n        super().__init__()\\n        self.features = nn.Sequential(\\n            nn.Linear(100, 128),\\n            nn.ReLU(),\\n            nn.Linear(128, 64),\\n            nn.ReLU()\\n        )\\n\\n    def forward(self, x):\\n        x = self.features(x)\\n        return x\\n\\n# Define the multi-task model\\nclass MultiTaskModel(nn.Module):\\n    def __init__(self, pre_trained_model):\\n        super().__init__()\\n        self.pre_trained_model = pre_trained_model\\n\\n        # Define the task-specific heads\\n        self.head1 = nn.Linear(64, 1)\\n        self.head2 = nn.Linear(64, 2)\\n\\n    def forward(self, x):\\n        x = self.pre_trained_model(x)\\n\\n        # Compute the predictions for each task\\n        y1 = self.head1(x)\\n        y2 = self.head2(x)\\n\\n        return y1, y2\\n\\n# Load the pre-trained model\\npre_trained_model = PreTrainedModel()\\npre_trained_model.load_state_dict(torch.load(\\'pre_trained_model.pt\\'))\\n\\n# Define the multi-task model\\nmulti_task_model = MultiTaskModel(pre_trained_model)\\n\\n# Train the multi-task model on multiple tasks simultaneously\\noptimizer = torch.optim.Adam(multi_task_model.parameters())\\n\\n# Define the loss function\\nloss_fn = nn.MSELoss()\\n\\n# Train the model for 100 epochs\\nfor epoch in range(100):\\n    # Load a batch of data for each task\\n    x1, y1 = ...\\n    x2, y2 = ...\\n\\n    # Compute the predictions for each task\\n    y1_pred, y2_pred = multi_task_model(x1, x2)\\n\\n    # Compute the loss\\n    loss = loss_fn(y1_pred, y1) + loss_fn(y2_pred, y2)\\n\\n    # Backpropagate the loss\\n    optimizer.zero_grad()\\n    loss.backward()\\n    optimizer.step()\\n\\n# Evaluate the multi-task model on each task\\ny1_pred, y2_pred = multi_task_model(x1, x2)\\n\\n# Compute the accuracy\\nacc1 = (y1_pred.argmax(1) == y1).float().mean()\\nacc2 = (y2_pred.argmax(1) == y2).float().mean()\\n\\nprint(\\'Accuracy on task 1:\\', acc1)\\nprint(\\'Accuracy on task 2:\\', acc2)\\n\\nDomain adaptation: In domain adaptation, the pre-trained model is adapted to a new domain. This can be done by adjusting the weights of the model’s layers or by using a technique called data augmentation.\\n\\nimport numpy as np\\nimport tensorflow as tf\\n\\n# Load the source and target datasets\\n(x_source, y_source), (x_target, y_target) = tf.keras.datasets.mnist.load_data()\\n\\n# Preprocess the data\\nx_source = x_source / 255.\\nx_target = x_target / 255.\\n\\n# Define the pre-trained model\\nmodel = tf.keras.models.Sequential([\\n  tf.keras.layers.Flatten(input_shape=(28, 28)),\\n  tf.keras.layers.Dense(128, activation=\\'relu\\'),\\n  tf.keras.layers.Dense(10, activation=\\'softmax\\')\\n])\\n\\n# Compile the model\\nmodel.compile(optimizer=\\'adam\\',\\n              loss=\\'sparse_categorical_crossentropy\\',\\n              metrics=[\\'accuracy\\'])\\n\\n# Train the model on the source dataset\\nmodel.fit(x_source, y_source, epochs=10)\\n\\n# Adapt the model to the target domain using data augmentation\\ndata_augmentation = tf.keras.Sequential([\\n  tf.keras.layers.RandomFlip(\\'horizontal\\'),\\n  tf.keras.layers.RandomRotation(0.2),\\n])\\n\\n# Adapt the model to the target dataset\\nmodel.fit(data_augmentation(x_target), y_target, epochs=10)\\n\\n# Evaluate the model on the target dataset\\ntest_loss, test_accuracy = model.evaluate(x_target, y_target)\\n\\nprint(\\'Test loss:\\', test_loss)\\nprint(\\'Test accuracy:\\', test_accuracy)\\n\\n# Zero-shot learning: In zero-shot learning, the model is trained on a set of classes that it has never seen before. \\n# This is accomplished through the use of a method known as word embedding.\\n\\nWhat Is a Pre-Trained Model?\\n\\nA pre-trained model is a machine learning model that has been trained on a large dataset of data. This dataset is typically much larger than the dataset that will be used to train the final model. The pre-trained model learns to extract features from the data, and these features can be used to train the final model more quickly and efficiently.\\n\\nPopular Pre-Trained Architectures\\n\\nThere are many popular pre-trained architectures, but some of the most common include:\\n\\nVGG (Visual Geometry Group): VGG is a family of convolutional neural networks that were first introduced in 2014. They are known for their simplicity and efficiency, and they have been used for a variety of tasks, including image classification, object detection, and segmentation.\\n\\nResNet (Residual Network): ResNet is a family of convolutional neural networks that were introduced in 2015. They are known for their ability to learn deeper features, and they have been shown to achieve state-of-the-art results on a variety of tasks.\\n\\nBERT (Bidirectional Encoder Representations from Transformers): BERT is a language model that was introduced in 2018. It is known for its ability to learn long-range dependencies in text, and it has been used for a variety of natural language processing tasks, including question-answering, sentiment analysis, and text summarization.\\n\\nThese are just a few of the many popular pre-trained architectures. The best architecture for a particular task will depend on the specific requirements of the task.\\n\\nWhat is Transferable Knowledge?\\n\\nTransferable knowledge in the context of transfer learning refers to the information, patterns, and representations learned by a machine learning model during training on one task or domain that can be effectively applied to improve performance on a different, often related, task or domain.\\n\\nLow-level Features vs. High-level Semantics\\n\\nIn transfer learning, models can transfer both low-level features and high-level semantics from a source task/domain to a target task/domain.\\n\\n1. Low-level Features: These are basic, primitive, and often raw data representations. In image processing, low-level features could include edges, colors, textures, or local patterns. In natural language processing, they might involve word embeddings or character-level features. Low-level features capture fundamental characteristics of the data that are often reusable in various contexts.\\n\\n2. High-level Semantics: High-level semantics represent more abstract and complex concepts or meanings within the data. In image analysis, high-level semantics could involve recognizing objects, scenes, or context. In natural language understanding, it might entail understanding sentiment, intent, or named entities.\\n\\nDomain-specific vs. Generic Knowledge\\n\\nTransfer learning can involve transferring both domain-specific and generic knowledge.\\n\\n1. Domain-specific Knowledge: This refers to knowledge and information that is specific to a particular task or domain. For example, medical image analysis might involve domain-specific knowledge related to anatomical structures or disease patterns. Transfer learning can adapt such knowledge to improve performance on similar medical imaging tasks.\\n\\n2. Generic Knowledge: Generic knowledge, on the other hand, is more general and broadly applicable. It includes foundational concepts and representations that are not tied to a specific task or domain. For instance, knowledge about spatial relationships, language syntax, or image textures can be considered generic. This type of knowledge can be transferred across a wide range of tasks and domains to aid in learning.\\n\\nTask Similarity & Domains\\n\\nHigh Task Similarity: When two tasks are highly similar, it means their objectives and desired outputs are closely related. For example, recognizing different breeds of dogs and recognizing different species of cats have high task similarity because they both involve image classification tasks related to animals.\\n\\nLow Task Similarity: In contrast, low task similarity indicates that the two tasks have distinct objectives. For instance, sentiment analysis of product reviews and image recognition of landmarks have low task similarity as they involve different data types and objectives.\\n\\nHigh Domain Similarity: High domain similarity suggests that two datasets or data distributions are similar in terms of their characteristics, such as data format, modality, or context. For example, medical X-ray images from different hospitals may have high domain similarity if they share similar image quality and medical conditions.\\n\\nLow Domain Similarity: Low domain similarity means that the datasets are dissimilar. For instance, medical X-ray images and satellite images of landscapes have low domain similarity because they come from entirely different domains with distinct data characteristics.\\n\\nFine-tuning vs. Feature Extraction\\n\\nIn transfer learning, fine-tuning and feature extraction are two common techniques to adapt a pre-trained model to a new task or domain:\\n\\nFine-tuning: Fine-tuning involves taking a pre-trained model (often on a source task) and training it further on a target task. During fine-tuning, the model’s weights are updated using the target task’s data while retaining some knowledge from the source task. Fine-tuning is particularly useful when the source and target tasks are closely related.\\n\\nFeature Extraction: Feature extraction refers to using a pre-trained model as a fixed feature extractor. Instead of modifying the model’s weights, the model is used to extract relevant features from the input data. These features can then be fed into a new classifier or model specific to the target task.\\n\\nSame-domain vs. Cross-domain Transfer\\n\\nSame-domain and cross-domain transfer refer to the relationship between the source and target domains in transfer learning:\\n\\nSame-domain Transfer: In same-domain transfer, the source and target domains are closely related or identical. This means that the data distribution, data characteristics, and context are similar between the source and target domains. Same-domain transfer is typically easier and tends to yield better results because the knowledge learned in one domain can be directly applied to the other.\\n\\nCross-domain Transfer: Cross-domain transfer involves transferring knowledge from a source domain to a target domain that is different. This type of transfer learning is more challenging because differences in data distribution, data modality, or context must be addressed. Techniques like domain adaptation and fine-tuning are often used to bridge the gap between the source and target domains in cross-domain transfer.\\n\\nSteps to Implement Transfer Learning\\n\\nHere are the key steps to implement transfer learning in a nutshell:\\n\\n1. Dataset Preparation: Begin by collecting and preprocessing your dataset. Ensure it is well-organized and contains labeled data for your target task. Typically, transfer learning works best when you have a smaller dataset, and it’s essential to split it into training, validation, and testing sets.\\n\\n2. Model Selection & Architecture: Choose a pre-trained deep learning model that suits your problem. Common choices include VGG, ResNet, Inception, and BERT, depending on whether it’s an image classification, object detection, or natural language processing task. Next, adapt the architecture of the selected model to match the number of classes or labels in your dataset.\\n\\n3. Transfer Strategy: Decide on the transfer learning strategy. There are two primary approaches which are: feature extraction and fine-tuning.\\n\\n4. Hyperparameter Tuning: Optimize hyperparameters such as learning rate, batch size, and the number of epochs. You can use techniques like grid search or random search to find the best combination of hyperparameters. This step is crucial for achieving optimal model performance.\\n\\n5. Training & Evaluation: Train the modified model on your dataset using the chosen hyperparameters. Monitor the training process by tracking metrics on the validation set. To prevent overfitting, consider using techniques like early stopping or dropout. Finally, evaluate the model’s performance on the test set using appropriate evaluation metrics, depending on your task (accuracy, F1-score, mean squared error, etc.).\\n\\nChallenges and Considerations\\n\\nWhile it offers numerous advantages, it also comes with its own set of challenges and considerations that practitioners must be aware of.\\n\\n1. Dataset Bias & Mismatch: Transfer learning relies heavily on the assumption that the source and target domains share some similarities. However, in real-world scenarios, datasets can exhibit bias or mismatch in terms of distribution, quality, or context.\\n\\n2. Overfitting & Generalization: One of the key challenges in transfer learning is finding the right balance between overfitting and generalization. Transferring too much knowledge from the source domain may lead to overfitting on the target domain, while transferring too little may hinder generalization.\\n\\n3. Catastrophic Forgetting: Transfer learning often involves fine-tuning a pre-trained model on a new task. However, this process can lead to the phenomenon known as catastrophic forgetting, where the model forgets important information from the source domain while adapting to the target domain. Strategies like progressive learning or elastic weight consolidation can help alleviate this issue.\\n\\n4. Ethical & Privacy Concerns: Transfer learning can be problematic when dealing with sensitive data, such as medical records or personal information. Care must be taken to ensure that transferred knowledge doesn’t violate privacy or ethical standards.\\n\\n5. Computational Resources: Large pre-trained models, such as those in natural language processing, demand substantial computational resources for training and fine-tuning. Smaller organizations or researchers with limited access to high-performance computing may face challenges in implementing transfer learning effectively.\\n\\nPractical Applications of Transfer Learning\\n\\nHere, we explore some practical applications of transfer learning in different fields with code examples:\\n\\nImage Classification: Transfer learning can be used to train models to classify images of different objects, such as cats, dogs, cars, and furniture. This can be used in a variety of applications, such as product search, image tagging, and medical imaging.\\n\\nimport tensorflow as tf\\n\\n# Load the pre-trained model\\nmodel = tf.keras.applications.MobileNetV2(weights=\\'imagenet\\')\\n\\n# Freeze the lower layers\\nfor layer in model.layers[:10]:\\n    layer.trainable = False\\n\\n# Add a new trainable layer for classification\\nnew_layer = tf.keras.layers.Dense(10, activation=\\'softmax\\')\\nmodel.add(new_layer)\\n\\n# Compile the model\\nmodel.compile(optimizer=\\'adam\\', loss=\\'sparse_categorical_crossentropy\\', metrics=[\\'accuracy\\'])\\n\\n# Train the model on your dataset\\n# ...\\n\\n# Evaluate the model on your test dataset\\n# ...\\n\\n# Save the model\\nmodel.save(\\'my_model.h5\\')\\n\\nNatural Language Processing: Transfer learning can be used to train models to perform natural language processing tasks, such as text classification, sentiment analysis, and machine translation. This can be used in a variety of applications, such as spam filtering, customer service, and social media analysis.\\n\\nimport tensorflow as tf\\n\\n# Load the pre-trained model\\nmodel = tf.keras.applications.bert.BertModel.from_pretrained(\\'bert-base-uncased\\')\\n\\n# Add a new trainable layer for sentiment analysis\\nnew_layer = tf.keras.layers.Dense(2, activation=\\'softmax\\')\\nmodel.add(new_layer)\\n\\n# Compile the model\\nmodel.compile(optimizer=\\'adam\\', loss=\\'sparse_categorical_crossentropy\\', metrics=[\\'accuracy\\'])\\n\\n# Train the model on your dataset\\n# ...\\n\\n# Evaluate the model on your test dataset\\n# ...\\n\\n# Make a prediction for a sentence\\nsentence = \\'I love this movie!\\'\\n\\n# Tokenize the sentence\\ntokens = model.tokenize(sentence)\\n\\n# Encode the tokens to hidden states\\nhidden_states = model([tokens])\\n\\n# Extract the output of the last layer\\noutput = hidden_states[:, -1, :]\\n\\n# Make a prediction\\nprediction = new_layer(output)\\n\\n# Print the prediction\\nprint(prediction)\\n\\nRecommendation Systems: Transfer learning can be used to train models to recommend products, movies, and other items to users. This can be used to improve the user experience on e-commerce websites and streaming services.\\n\\nimport tensorflow as tf\\n\\n# Load the pre-trained model\\nmodel = tf.keras.models.load_model(\\'my_model.h5\\')\\n\\n# Make a prediction for a user\\nuser_id = 1\\nitem_ids = [2, 3, 4]\\n\\npredictions = model.predict([user_id, item_ids])\\n\\n# Recommend the top 5 items\\nrecommended_item_ids = predictions.argsort()[::-1][:5]\\n\\n# Print the recommended items\\nprint(recommended_item_ids)\\n\\nFraud Detection: Transfer learning can be used to train models to detect fraudulent transactions and other types of fraud. This can be used to protect businesses and consumers from financial losses.\\n\\nimport tensorflow as tf\\n\\n# Load the pre-trained model\\nmodel = tf.keras.models.load_model(\\'my_model.h5\\')\\n\\n# Make a prediction for a transaction\\ntransaction_data = {\\n    \\'amount\\': 1000,\\n    \\'merchant\\': \\'Amazon\\',\\n    \\'card_type\\': \\'Visa\\',\\n    \\'country\\': \\'US\\'\\n}\\n\\nprediction = model.predict(transaction_data)\\n\\n# If the prediction is greater than 0.5, the transaction is likely to be fraudulent\\nif prediction > 0.5:\\n    print(\\'The transaction is likely to be fraudulent.\\')\\nelse:\\n    print(\\'The transaction is likely to be legitimate.\\')\\n\\nMedical Diagnosis: Transfer learning can be used to train models to diagnose diseases and other medical conditions. This can be used to improve the accuracy and efficiency of medical diagnosis.\\n\\nimport tensorflow as tf\\n\\n# Load the pre-trained model\\nmodel = tf.keras.models.load_model(\\'my_model.h5\\')\\n\\n# Make a prediction for an image\\nimage_path = \\'chest_xray.jpg\\'\\n\\nimage = tf.keras.preprocessing.image.load_img(image_path, target_size=(224, 224))\\nimage = tf.keras.preprocessing.image.img_to_array(image)\\nimage = tf.expand_dims(image, axis=0)\\n\\nprediction = model.predict(image)\\n\\n# If the prediction is greater than 0.5, the image is likely to show pneumonia\\nif prediction > 0.5:\\n    print(\\'The image is likely to show pneumonia.\\')\\nelse:\\n    print(\\'The image is likely to be normal.\\')\\n\\nConclusion\\n\\nThrough this guide, we’ve explored all you need to know about transfer learning, including different transfer learning scenarios, popular pre-trained models, and the steps involved in adapting them for various tasks with code examples.\\n\\nArtificial Intelligence\\n\\nProgramming\\n\\nMachine Learning\\n\\nLlm\\n\\n--\\n\\n--\\n\\nWritten by David Fagbuyiro\\n\\n23 Followers\\n\\nTechnical writer\\n\\nHelp\\n\\nStatus\\n\\nAbout\\n\\nCareers\\n\\nPress\\n\\nBlog\\n\\nPrivacy\\n\\nTerms\\n\\nText to speech\\n\\nTeams'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = data[0].page_content\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "splitter = CharacterTextSplitter(\n",
    "    separator='\\n',\n",
    "    chunk_size= 1000,\n",
    "    chunk_overlap=0,\n",
    "\n",
    ")\n",
    "\n",
    "chunks = splitter.split_text(text)\n",
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Open in app\\nSign in\\nWrite\\nSign in\\nGuide To Transfer Learning in Deep Learning\\nDavid Fagbuyiro\\nFollow\\n14 min read\\nApr 20, 2024\\n--\\nIn this guide, we will cover what transfer learning is, and the main approaches to transfer learning in deep learning.\\nWhat is Transfer Learning\\nTransfer learning is an approach to machine learning where a model trained on one task is used as the starting point for a model on a new task. This is done by transferring the knowledge that the first model has learned about the features of the data to the second model.\\nIn deep learning, transfer learning is often used to solve problems with limited data. This is because deep learning models typically require a large amount of data to train, which can be difficult or expensive to obtain.\\nWhy Use Transfer Learning?\\nHere are some reasons why you might want to use transfer learning:'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "861\n",
      "876\n",
      "958\n",
      "981\n",
      "998\n",
      "989\n",
      "931\n",
      "848\n",
      "989\n",
      "962\n",
      "955\n",
      "675\n",
      "835\n",
      "795\n",
      "925\n",
      "790\n",
      "859\n",
      "827\n",
      "796\n",
      "847\n",
      "956\n",
      "981\n",
      "922\n",
      "977\n",
      "864\n",
      "786\n",
      "427\n"
     ]
    }
   ],
   "source": [
    "for chunk in chunks:\n",
    "    print(len(chunk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "r_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=['\\n\\n','\\n', ' ' ],\n",
    "    chunk_size= 1000,\n",
    "    chunk_overlap=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks = r_splitter.split_text(text)\n",
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "876\n",
      "880\n",
      "967\n",
      "961\n",
      "853\n",
      "840\n",
      "932\n",
      "915\n",
      "946\n",
      "918\n",
      "752\n",
      "787\n",
      "873\n",
      "795\n",
      "762\n",
      "916\n",
      "959\n",
      "921\n",
      "819\n",
      "872\n",
      "963\n",
      "990\n",
      "917\n",
      "811\n",
      "889\n",
      "965\n",
      "699\n"
     ]
    }
   ],
   "source": [
    "for chunk in chunks:\n",
    "    print(len(chunk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "7\n",
      "5\n",
      "7\n",
      "43\n",
      "15\n",
      "6\n",
      "11\n",
      "12\n",
      "2\n",
      "118\n",
      "25\n",
      "271\n",
      "221\n",
      "26\n",
      "66\n",
      "254\n",
      "277\n",
      "264\n",
      "26\n",
      "51\n",
      "247\n",
      "8\n",
      "130\n",
      "146\n",
      "124\n",
      "118\n",
      "124\n",
      "38\n",
      "11\n",
      "3\n",
      "199\n",
      "302\n",
      "122\n",
      "87\n",
      "181\n",
      "60\n",
      "65\n",
      "90\n",
      "205\n",
      "56\n",
      "102\n",
      "81\n",
      "242\n",
      "147\n",
      "69\n",
      "147\n",
      "69\n",
      "50\n",
      "176\n",
      "170\n",
      "179\n",
      "34\n",
      "274\n",
      "71\n",
      "183\n",
      "116\n",
      "63\n",
      "101\n",
      "21\n",
      "136\n",
      "82\n",
      "121\n",
      "49\n",
      "132\n",
      "91\n",
      "77\n",
      "95\n",
      "88\n",
      "118\n",
      "69\n",
      "204\n",
      "42\n",
      "118\n",
      "75\n",
      "218\n",
      "141\n",
      "80\n",
      "190\n",
      "99\n",
      "104\n",
      "69\n",
      "193\n",
      "28\n",
      "346\n",
      "33\n",
      "86\n",
      "281\n",
      "251\n",
      "334\n",
      "165\n",
      "31\n",
      "291\n",
      "43\n",
      "141\n",
      "392\n",
      "317\n",
      "37\n",
      "86\n",
      "349\n",
      "419\n",
      "25\n",
      "314\n",
      "273\n",
      "336\n",
      "267\n",
      "34\n",
      "136\n",
      "350\n",
      "305\n",
      "37\n",
      "123\n",
      "391\n",
      "420\n",
      "36\n",
      "68\n",
      "300\n",
      "372\n",
      "143\n",
      "279\n",
      "421\n",
      "29\n",
      "137\n",
      "257\n",
      "306\n",
      "396\n",
      "252\n",
      "326\n",
      "43\n",
      "105\n",
      "256\n",
      "23\n",
      "90\n",
      "85\n",
      "127\n",
      "113\n",
      "39\n",
      "47\n",
      "42\n",
      "310\n",
      "23\n",
      "110\n",
      "130\n",
      "113\n",
      "39\n",
      "47\n",
      "66\n",
      "57\n",
      "68\n",
      "71\n",
      "50\n",
      "40\n",
      "221\n",
      "23\n",
      "78\n",
      "63\n",
      "48\n",
      "82\n",
      "57\n",
      "198\n",
      "23\n",
      "78\n",
      "151\n",
      "44\n",
      "224\n",
      "193\n",
      "23\n",
      "78\n",
      "62\n",
      "176\n",
      "33\n",
      "204\n",
      "10\n",
      "235\n",
      "23\n",
      "11\n",
      "16\n",
      "3\n",
      "2\n",
      "2\n",
      "26\n",
      "12\n",
      "16\n",
      "4\n",
      "6\n",
      "5\n",
      "7\n",
      "5\n",
      "4\n",
      "7\n",
      "5\n",
      "14\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "chunks = text.split('\\n\\n')\n",
    "for chunk in chunks:\n",
    "    print(len(chunk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In deep learning, transfer learning is often used to solve problems with limited data. This is because deep learning models typically require a large amount of data to train, which can be difficult or expensive to obtain.'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_chunk = chunks[13]\n",
    "first_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gen-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
